from copy import deepcopy
import numpy as np
import utils
import E_estimate
import I_estimate
import P_estimate
import S_estimate
import math

#from E_estimate import Es_estimate
#from I_estimate import Is_estimate,Ic_estimate
#from P_estimate import P_estimate
#from S_estimate import S_estimate
from scipy.stats import multivariate_normal
from scipy.stats import norm
"""
Implemetationof a particle filter.
Data required:
N_particles,
System model,
Measuremet model

Steps:
Generate or get the initial distribution of state variables. P(x0)

predict step:
predict P(x1) from P(x0) using 

update step:

resample particles


"""

"""
for initial distribution of state, ie x0 , I will provide a function which accepts initial parameters for x0
    and give probablity density function for specific values of x0 (x0 = alpha) as follows:

    1) if state vector alpha exactly matches the initial parameters, it outputs 1,
    2) 0 otherwise.

I think this is a reasonable distribution to start with. As more measurements are taken, initial distribution
does not matter much.
"""
#N_particles = 400

particles = [] # a list of state vectors

class particle_filter:
    def __init__(self,ved,intersection_info, n_particles,initial_course, initial_measurement,interval,pose_covariance, speed_deviation):
        

        #setup the place
        self.particles = []
        self.weights = []
        #after this, the filter has particles initialized with initial state vector
        self.n_particles = n_particles
        self.ved  = ved #the vehicles entering direction.
        self.intersection_info = intersection_info
        #self.interval = interval  #varying time interval modification 
        self.position_covariance =  pose_covariance[np.ix_([0,2],[0,2])]
        self.theta_deviation = pose_covariance[2][2]
        self.pose_covariance  = pose_covariance
        self.speed_deviation = speed_deviation


        #self.evolution_model = evolution_model
        #self.measurement_model = measurement_model

        #the following variables are used to compute on the posterior distribution to calculate the risk
        # self.particles was not used to compute because it will contain particles after resampling step
        # and the weights corresponding to the resampled particles will not match the current weight.
        # I discussed with Emelie and she suggested to compute on posterior on particles after
        # the update step, and before resampling. 
        self.posterior_particles = [] 
        self.posterior_weights = []

        self.neff_threshold = self.n_particles / 2
        p = initial_measurement[:3]
        s = initial_measurement[-1]
        initial_state_vector = state_vector(1,1,initial_course,p,s)

        #this generation of particles is based on a delta function of initial_state_vector
        self.posterior_particles = utils.generate_inital_particles(initial_state_vector,n_particles,self.pose_covariance,self.speed_deviation)
        self.particles = utils.generate_inital_particles(initial_state_vector,n_particles,self.pose_covariance,self.speed_deviation)

        #weight at this step would be 1/n_particles because each of the state are generated by probablity 1
        #and we divide by number of particels to normalize, given each particle same weight
        self.posterior_weights = [1/self.n_particles]*self.n_particles
        self.weights = [1/self.n_particles]*self.n_particles

    def neff(self,weights):
        return 1. / np.sum(np.square(weights))

    
    def get_most_likely_state(self):
        #there are two methods to get this particle. this is addressed in github issue #10
        #first way:
        # i = np.argmax(self.posterior_weights)
        # return self.posterior_particles[i]

        #second way:
        #get Es
        es_ = [x.Es for x in self.posterior_particles]
        blob = zip(es_,self.posterior_weights)
        #get weights of particles where es = stop
        stop_weights = sum([x[1] for x in filter(lambda x: x[0] == 0,blob)])

        #get weights of particles where es = go
        go_weights = sum([x[1] for x in filter(lambda x: x[0] == 1,blob)])
        newEs = np.argmax([stop_weights,go_weights])

        #get Is
        is_ = [x.Is for x in self.posterior_particles]
        blob = zip(is_,self.posterior_weights)
        #get weights of particles where es = stop
        stop_weights = sum([x[1] for x in filter(lambda x: x[0] == 0,blob)])

        #get weights of particles where es = go
        go_weights = sum([x[1] for x in filter(lambda x: x[0] == 1,blob)])
        newIs = np.argmax([stop_weights,go_weights])

        #get Ic
        ic_ = [x.Ic for x in self.posterior_particles]
        blob = zip(ic_,self.posterior_weights)
        uturn_weights = sum([x[1] for x in filter(lambda x: x[0] == 0,blob)])
        left_turn_weights = sum([x[1] for x in filter(lambda x: x[0] == 1,blob)])
        straight_ahead_weights = sum([x[1] for x in filter(lambda x: x[0] == 2,blob)])
        right_turn_weights = sum([x[1] for x in filter(lambda x: x[0] == 3,blob)])

        newIc = np.argmax([uturn_weights, left_turn_weights, straight_ahead_weights,right_turn_weights])

        i = np.argmax(self.posterior_weights)
        newP = self.posterior_particles[i].P
        newS = self.posterior_particles[i].S

        return state_vector(newEs,newIs,newIc,newP,newS)

    def step_time(self, measurement_vector, courses,poses,speeds,ved,t,interval):
        """
        for each time step k, use the evolution model on each particle to get the density on the kth step
        and sample a particle on each density we obtained
        """
        new_particles = []
        for p in self.particles:
            #for each particle, generate a new particle by sampling a  value for each variable in the particle
            E_density = E_estimate.Es_estimate(p.P,p.S,p.Ic,self.ved,self.intersection_info,courses,poses,speeds,t,interval,ved)
            new_Es = np.random.choice([0,1],p=E_density)

            Is_density = I_estimate.Is_estimate(p.Is,new_Es)
            new_Is = np.random.choice([0,1],p=Is_density)

            #number of courses is 4. todo: replace with enum or variable
            Ic_density = I_estimate.Ic_estimate(p.Ic,4)
            #here also use enum. todo
            new_Ic = np.random.choice([0,1,2,3],p=Ic_density)

            new_P = P_estimate.P_estimate(p.P,p.S,p.Ic,self.intersection_info,interval,self.ved,self.pose_covariance) #in here, a sample is returned
            new_S = S_estimate.S_estimate(self.intersection_info,p.S,p.P,new_Ic,new_Is,interval,self.speed_deviation) #here also we get a sample instead of density

            new_particle = state_vector(new_Es,new_Is,new_Ic,new_P,new_S)
            new_particles.append(new_particle)
            #new_p = self.get_next_particle(p,self.ved)
            
        #check if the weight calculation depends on previous weight. todo
        weights = []
        for p in new_particles:
            weights.append(self.likelihood(p,measurement_vector))
        
        #normalize
        w_sum = float(sum(weights))
        if w_sum != 0:
            weights = list(map(lambda x: x / w_sum,weights))
            self.posterior_particles = new_particles
            self.posterior_weights = weights
        else:
            #if all particles have zero weight. we ignore.
            # but the thing is this is not supposed to happen. having particles with all zero weights
            # indicate that the particles were nowhere near the actual state vector and this is a very
            # bad thing. Sadly I have encountered this case while simulation which the particles
            # ran off to somewhere else other than the state vector. 
            # have to fix this sometime later
            self.particles = new_particles
            self.weights = weights
            return

        #resample particles by their weight:
        #todo: neeed to verify if this is correct
        #sources obtained: 
        # for neff comparision: https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python/blob/master/12-Particle-Filters.ipynb
        # This is a sampling importance resampling. 
        # http://people.math.aau.dk/~kkb/Undervisning/Bayes14/sorenh/docs/sampling-notes.pdf
        if self.neff(weights) < self.neff_threshold:
            #print("resampled")
            new_particles = np.random.choice(new_particles,self.n_particles, p=weights)
        
        self.particles = new_particles
        self.weights = weights


    def state_vector_density(self ):
        pass
    
    def likelihood(self,p, measurement_vector):
        #multiplying the likelihood of each variable here. i have to check if this is the right way
        #todo
        #measured pose == > measurement_vector[:3]
        #paper mentions no correlation between x,y,theta
        #used this: https://blogs.sas.com/content/iml/2010/12/10/converting-between-correlation-and-covariance-matrices.html
        #pose_cov = np.array([[1,0,0],[0,1,0],[0,0,1]])
        #pose_likelihood = multivariate_normal.pdf(measurement_vector[:3],mean=p.P,cov=self.pose_covariance)
        #pose_likelihood = 1
        #dev = self.pose_covariance[0][0]
        #for n,i in enumerate(p.P):
        #    pose_likelihood *= norm(i,dev).pdf(measurement_vector[n])


        #in the likelihood of pose, the x and y are linear, and pdf calculation for a normal distribution is okay
        # but theta is periodic. angles repeat after 2pi. 
        # if there is a distribution with mean , say 350, under a non periodic normal distribution,
        # obtaining 0 is quite unlikely. but as theta is periodic and 0 angle is just 5 degrees away from 350
        # we would expect a higher likelihood for that.
        # solution would be to use a wrapped normal distribution as per : https://en.wikipedia.org/wiki/Wrapped_normal_distribution
        # this maybe computationaly expensive and right now i am using a simplified version of it.
        #pose_likelihood = multivariate_normal.pdf(measurement_vector[:3],mean=p.P,cov=self.pose_covariance)
        #https://math.stackexchange.com/questions/892832/why-we-consider-log-likelihood-instead-of-likelihood-in-gaussian-distribution
        #calculating this likelihood takes a lot of time as per benchmark profile output
        #https://stats.stackexchange.com/questions/201545/likelihood-calculation-in-particle-filtering
        position_likelihood = multivariate_normal.pdf(measurement_vector[:2],mean=p.P[:2],cov=self.position_covariance)
        #pose_likelihood = multivariate_normal.pdf(measurement_vector[:3],mean=p.P,cov=self.pose_covariance)
        #print("multivariate likelihood= " +str(position_likelihood))
        #print("measured: " + str(measurement_vector[:2]))
        #print("particle have: " + str(p.P[:2]))

        # x_likelihood = norm.pdf(measurement_vector[0],loc=p.P[0],scale=math.sqrt(self.pose_covariance[0][0]))
        # y_likelihood = norm.pdf(measurement_vector[1],loc=p.P[1],scale=math.sqrt(self.pose_covariance[1][1]))

        # position_likelihood = x_likelihood * y_likelihood
        # print("independent likelihood= " +str(position_likelihood))
        

        mean_theta = p.P[2]
        upper_bound = mean_theta + math.pi
        lower_bound = mean_theta - math.pi
        measured_angle = measurement_vector[2]
        while (measured_angle < lower_bound or measured_angle > upper_bound):
            if (measured_angle) < lower_bound:
                measured_angle = measured_angle + math.pi*2
            if (measured_angle > upper_bound):
                measured_angle = measured_angle - math.pi*2


        #opposite_end = (mean_theta - math.pi) % (2*math.pi)
        #delta = abs(measurement_vector[2] - opposite_end)

        theta_likelihood = norm.pdf(measured_angle,loc=mean_theta,scale=self.theta_deviation)
        #pose_likelihood = position_likelihood * theta_likelihood # position and angle are independent so this multiplication is okay
        pose_likelihood = position_likelihood * theta_likelihood # position and angle are independent so this multiplication is okay


        speed_likelihood = norm.pdf(measurement_vector[3],loc=p.S,scale=self.speed_deviation)

        return pose_likelihood * speed_likelihood
    
    def get_next_particle(self,t,interval, measurement_vector,courses,speeds,ved):
        #not implemented to avoid performance degradation for function call
        return
        #for each particle,
        # sample a partle from the density which we get from the evolution model:
        #caveat: courses already have this vehicle in the list.
        #this can lead to having more vehicles than required. things become doubled :/
        #todo: fix this, 
    
    def get_highest_n_particles(self,n):
        sorted_indexes = np.argsort(self.weights)
        return [self.particles[x] for x in sorted_indexes[-n:]]
        #select n indexes from behind:
        









class state_vector:
    #self.Es =0 # expected longitudinal motion, discrete value {0, 1} 
    #self.Is =0 # Intended longitudinal motion, discrete value {0, 1} 
    #self.Ic = 0 # intended course, discrete , {0,1,2,3} or more if intersection has more courses
    #self.P = (0,0,0) # actual pose, x, y, theta
    #self.S = 0 #actual speed

    #perhaps measured poses/ speed may not be part of state vector, so im commenting out
#    self.Pm = (0,0,0) # measured pose, x, y theta
#    self.Sm = 0 #measured speed
    
    def __init__(self,es,is_,ic,p,s):
        self.Es = es # expected longitudinal motion, discrete value {0, 1} 
        self.Is = is_ # Intended longitudinal motion, discrete value {0, 1} 
        self.Ic = ic # intended course, discrete , {0,1,2,3} or more if intersection has more courses
        self.P = p   #pose
        self.S = s   #speed
#        self.Pm =pm
#        self.Sm = sm


    def equals(self,state_vector2):
        return self.P  == state_vector2.P  and \
               self.S  == state_vector2.S  and \
               self.Is == state_vector2.Is  and \
               self.Ic == state_vector2.Ic  and \
               self.Es == state_vector2.Es  
               # self.Pm == state_vector2.Pm and
               #self.Sm == state_vector2.Sm  and \


#def filter_step(particles, measurement_function):
#    initial_statevec = []
#    x_k = [] # particles, a list of state vectors
#    weights = [0]*N_particles
#    #first, generate from initial distribution
#    x_k = utils.generate_inital_particles(initial_statevec,N_particles)
#